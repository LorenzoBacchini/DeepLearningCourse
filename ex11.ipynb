{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "734aeaed-1d0b-415d-bdaf-d27101e1c112",
   "metadata": {},
   "source": [
    "# U-Net for Image Segmentation\n",
    "dataset url: https://univpr-my.sharepoint.com/:u:/g/personal/tomaso_fontanini_unipr_it/ESWu7tAMzlFMnraBhlX29IUBrBQzCyQtJOEOjEcqoFvn8g?e=whclvZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a4acd8-a424-4029-add8-7961c2be589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from res.plot_lib import plot_data, plot_data_np, plot_model, set_default\n",
    "set_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5598e934-793c-4503-bf80-cbb0ed10410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Settings and Hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "bs_train, bs_test = 4, 1\n",
    "epochs = 100\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3663392-0ea0-432b-a392-11d3f0687da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ds list\n",
    "\n",
    "#Create lists for the paths of training images and training masks \n",
    "img_files = []\n",
    "mask_files = glob('C:/Users/Tomaso Fontanini/Documents/Teaching/Dataset/brain_mri/lgg-mri-segmentation/kaggle_3m/*/*_mask*')\n",
    "\n",
    "for i in mask_files:\n",
    "    img_files.append(i.replace('_mask',''))\n",
    "\n",
    "print(mask_files[0])\n",
    "print(img_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0cd5d5-4736-4e01-bf26-d20fc6d21051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image function\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        if np.array(img).ndim != 3:\n",
    "            return img.convert('L')\n",
    "        else:\n",
    "            return img.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ceedd0-e356-4ed7-9e3f-8ac45b2c1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display some samples\n",
    "rows,cols=3,3\n",
    "fig=plt.figure(figsize=(10,10))\n",
    "for i in range(1,rows*cols+1):\n",
    "    fig.add_subplot(rows,cols,i)\n",
    "    img_path=img_files[i]\n",
    "    msk_path=mask_files[i]\n",
    "    img=pil_loader(img_path)\n",
    "    msk=pil_loader(msk_path)\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(msk,alpha=0.4)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c144fc-2fef-4a6a-b405-2e1783f992f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset in train and test\n",
    "ds_split = len(img_files)//10\n",
    "\n",
    "img_files_train, mask_files_train = img_files[ds_split:], mask_files[ds_split:]\n",
    "img_files_test, mask_files_test = img_files[:ds_split], mask_files[:ds_split]\n",
    "\n",
    "print(len(img_files_train), len(img_files_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b577ddeb-14d0-417e-84ad-a443fb78ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a custom dataset class which applies identical transformations to the training and test data\n",
    "# In this case we will define transforms directly into the get_item function\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, image_paths, target_paths, train=True):\n",
    "        self.image_paths = image_paths\n",
    "        self.target_paths = target_paths\n",
    "\n",
    "    def transform(self, image, mask):\n",
    "        # Resize\n",
    "        resize = transforms.Resize(size=(256, 256))\n",
    "        image = resize(image)\n",
    "        mask = resize(mask)\n",
    "\n",
    "        # Random horizontal flipping (we apply transforms here because we need to apply \n",
    "        # them with the same probability to both img and mask)\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.hflip(image)\n",
    "            mask = TF.hflip(mask)\n",
    "\n",
    "        # Transform to tensor\n",
    "        image = TF.to_tensor(image)\n",
    "        mask = TF.to_tensor(mask)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_paths[index])\n",
    "        mask = Image.open(self.target_paths[index])\n",
    "        x, y = self.transform(image, mask)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f67113a-f9d8-482c-8acb-a2e64ae223be",
   "metadata": {},
   "source": [
    "We will use the Dice loss (smoothed).\n",
    "\n",
    "Alternative: Binary Cross Entropy or L1 Loss (try it as an exercise).\n",
    "\n",
    "![alt text](https://i.stack.imgur.com/OsH4y.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b39412-86e3-462a-b64c-1a76934090ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a dice loss function\n",
    "def dc_loss(pred, target):\n",
    "    smooth = 1.\n",
    "\n",
    "    predf = pred.view(-1)\n",
    "    targetf = target.view(-1)\n",
    "    intersection = (predf * targetf).sum()\n",
    "    \n",
    "    return 1 - ((2. * intersection + smooth) /\n",
    "              (predf.sum() + targetf.sum() + smooth))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed466fdc-b339-436f-ad68-5ec6870f60d2",
   "metadata": {},
   "source": [
    "### U-Net\n",
    "\n",
    "![alt text](https://www.researchgate.net/publication/331406702/figure/fig2/AS:731276273262594@1551361258173/Illustration-of-the-U-net-architecture-The-figure-illustrates-the-U-net-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2ff87b-73af-4a8e-a590-bb2a6b8c3404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the UNet architecture\n",
    "\n",
    "\n",
    "def conv_layer(input_channels, output_channels):     #This is a helper function to create the convolutional blocks\n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(output_channels, output_channels, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(output_channels),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    return conv\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.down_1 = conv_layer(3, 64)\n",
    "        self.down_2 = conv_layer(64, 128)\n",
    "        self.down_3 = conv_layer(128, 256)\n",
    "        self.down_4 = conv_layer(256, 512)\n",
    "        self.down_5 = conv_layer(512, 1024)\n",
    "        \n",
    "        self.up_1 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=2, stride=2)\n",
    "        self.up_conv_1 = conv_layer(1024, 512)\n",
    "        self.up_2 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=2, stride=2)\n",
    "        self.up_conv_2 = conv_layer(512, 256)\n",
    "        self.up_3 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=2, stride=2)\n",
    "        self.up_conv_3 = conv_layer(256, 128)\n",
    "        self.up_4 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=2)\n",
    "        self.up_conv_4 = conv_layer(128, 64)\n",
    "        \n",
    "        self.output = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1, padding=0)\n",
    "        self.output_activation = nn.Sigmoid()\n",
    "                \n",
    "    def forward(self, img):     #The print statements can be used to visualize the input and output sizes for debugging\n",
    "        x1 = self.down_1(img)\n",
    "        #print(x1.size())\n",
    "        x2 = self.max_pool(x1)\n",
    "        #print(x2.size())\n",
    "        x3 = self.down_2(x2)\n",
    "        #print(x3.size())\n",
    "        x4 = self.max_pool(x3)\n",
    "        #print(x4.size())\n",
    "        x5 = self.down_3(x4)\n",
    "        #print(x5.size())\n",
    "        x6 = self.max_pool(x5)\n",
    "        #print(x6.size())\n",
    "        x7 = self.down_4(x6)\n",
    "        #print(x7.size())\n",
    "        x8 = self.max_pool(x7)\n",
    "        #print(x8.size())\n",
    "        x9 = self.down_5(x8)\n",
    "        #print(x9.size())\n",
    "        \n",
    "        x = self.up_1(x9)\n",
    "        #print(x.size())\n",
    "        x = self.up_conv_1(torch.cat([x, x7], 1))\n",
    "        #print(x.size())\n",
    "        x = self.up_2(x)\n",
    "        #print(x.size())\n",
    "        x = self.up_conv_2(torch.cat([x, x5], 1))\n",
    "        #print(x.size())\n",
    "        x = self.up_3(x)\n",
    "        #print(x.size())\n",
    "        x = self.up_conv_3(torch.cat([x, x3], 1))\n",
    "        #print(x.size())\n",
    "        x = self.up_4(x)\n",
    "        #print(x.size())\n",
    "        x = self.up_conv_4(torch.cat([x, x1], 1))\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = self.output(x)\n",
    "        x = self.output_activation(x)\n",
    "        #print(x.size())\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9340ae-1cc3-4c3f-a366-7ce8d7c0980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets\n",
    "train_dataset = MRIDataset(img_files_train, mask_files_train)\n",
    "test_dataset = MRIDataset(img_files_test, mask_files_test)\n",
    "\n",
    "#Dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=bs_train, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=bs_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af22ec-911a-444f-8ad6-913db2cb68d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the model and optimizer\n",
    "model = UNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9,0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee1d271-fb37-4b34-a06e-81cbee7e3cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define path where to save the model\n",
    "PATH = './res/unet.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2df10d-c499-4c27-90b3-21a62f00b533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "#Training function \n",
    "def train(model, epochs):\n",
    "    \n",
    "    #Keep track of average training and test losses for each epoch\n",
    "    avg_train_losses = []\n",
    "    avg_test_losses = []\n",
    "    \n",
    "    #Trigger for earlystopping\n",
    "    earlystopping = False \n",
    "\n",
    "    #Training loop\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        #Record the training and test losses for each batch in this epoch\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        loop = tqdm(enumerate(train_dataloader), total = len(train_dataloader), leave = False)\n",
    "        for batch, (images, targets) in loop:\n",
    "            \n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device) # the ground truth mask\n",
    "\n",
    "            model.zero_grad()\n",
    "            pred = model(images)\n",
    "            loss = dc_loss(pred, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "                        \n",
    "            with torch.no_grad():     #Show some samples at the first batch of each epoch \n",
    "                if batch == 1:\n",
    "                    torch.save(model.state_dict(), PATH)\n",
    "\n",
    "                    model.eval()\n",
    "\n",
    "                    (img, mask) = next(iter(test_dataloader))\n",
    "                    img = img.to(device)\n",
    "                    mask = mask.to(device)\n",
    "                    mask = mask[0]\n",
    "                    pred = model(img)\n",
    "\n",
    "                    plt.figure(figsize=(12,12))\n",
    "                    plt.subplot(1,3,1)\n",
    "                    plt.imshow(np.squeeze(img.cpu().numpy()).transpose(1,2,0))\n",
    "                    plt.title('Original Image')\n",
    "                    plt.subplot(1,3,2)\n",
    "                    plt.imshow((mask.cpu().numpy()).transpose(1,2,0).squeeze(axis=2))\n",
    "                    plt.title('Original Mask')\n",
    "                    plt.subplot(1,3,3)\n",
    "                    plt.imshow(np.squeeze(pred.cpu()) > .5)\n",
    "                    plt.title('Prediction')\n",
    "                    plt.show()\n",
    "\n",
    "                    model.train()\n",
    "\n",
    "                \n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():     #Record and print average validation loss for each epoch \n",
    "            for test_batch, (test_images, test_targets) in enumerate(test_dataloader):\n",
    "                test_images = test_images.to(device)\n",
    "                test_targets = test_targets.to(device)\n",
    "                test_pred = model(test_images.detach())\n",
    "\n",
    "                test_loss = dc_loss(test_pred, test_targets).item()\n",
    "\n",
    "                test_losses.append(test_loss)\n",
    "\n",
    "            epoch_avg_train_loss = np.average(train_losses)\n",
    "            epoch_avg_test_loss = np.average(test_losses)\n",
    "            avg_train_losses.append(epoch_avg_train_loss)\n",
    "            avg_test_losses.append(epoch_avg_test_loss)\n",
    "\n",
    "            print_msg = (f'train_loss: {epoch_avg_train_loss:.5f} ' + f'valid_loss: {epoch_avg_test_loss:.5f}')\n",
    "\n",
    "            print(print_msg)\n",
    "        \n",
    "        if epoch > 5:     #Early stopping with a patience of 1 and a minimum of 5 epochs \n",
    "            if avg_test_losses[-1]>=avg_test_losses[-2]:\n",
    "                print(\"Early Stopping Triggered With Patience 1\")\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "                earlystopping = True \n",
    "        if earlystopping:\n",
    "            break\n",
    "\n",
    "    return  model, avg_train_losses, avg_test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fcfc7e-be29-488d-92eb-9e10f1e74951",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model \n",
    "best_model, avg_train_losses, avg_val_losses = train(model, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e68a4d-da0d-4448-b734-5cd4f0519d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load a set of weights\n",
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a06045e-20af-4b9e-ad6c-c10de925a945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the model with some samples from the test dataset \n",
    "model.eval()\n",
    "\n",
    "(img, mask) = next(iter(test_dataloader))\n",
    "img = img.to(device)\n",
    "mask = mask.to(device)\n",
    "mask = mask[0]\n",
    "pred = model(img)\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(np.squeeze(img.cpu().numpy()).transpose(1,2,0))\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow((mask.cpu().numpy()).transpose(1,2,0).squeeze(axis=2))\n",
    "plt.title('Original Mask')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(np.squeeze(pred.cpu()) > .5)\n",
    "plt.title('Prediction')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
