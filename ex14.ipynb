{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1279d67c-8a26-41c2-9cb8-4c2276911980",
   "metadata": {},
   "source": [
    "# Variational Autoencoder\n",
    "\n",
    "First generative network that we are going to see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12de35a-7b4b-4ee3-8124-88c512c0ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from res.plot_lib import plot_data, plot_data_np, plot_model, set_default\n",
    "set_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20014e0f-f9ec-45df-ba67-ee30155a30e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "# TRAIN dataset\n",
    "# Pytorch already has some datasets available for downlaod\n",
    "train_dataset = datasets.MNIST('../data', train=True, download=True, \n",
    "                # transforms that we want to apply when iterating the dataset\n",
    "                   transform=transforms.ToTensor())\n",
    "\n",
    "# The datasoar will iterate through the dataset and load the data in memory\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                        train_dataset,\n",
    "                        batch_size=64, shuffle=True)\n",
    "\n",
    "# TEST dataset\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, \n",
    "                   transform=transforms.ToTensor()),\n",
    "                    batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abf5535-e0ff-428f-ad4c-f0991eaf7e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some images\n",
    "to_pil = transforms.ToPILImage()\n",
    "plt.figure(figsize=(16, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    image, _ = train_loader.dataset.__getitem__(i) # load one image\n",
    "    plt.imshow(to_pil(image))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1265a4e-c748-4150-9d16-3e85576320b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052061f2-4654-42a3-96e7-6ad08c7fe822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's define our VAE\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_channels=1,\n",
    "        hidden_size=64,\n",
    "        latent_size=2\n",
    "    ):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "        ## encoder ##\n",
    "        self.Encoder = nn.Sequential(nn.Conv2d(1, 16, 4, stride = 2, padding=1), #16x14x14   [(Wâˆ’K+2P)/S]+1\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Conv2d(16, 32, 4, stride = 2, padding=1), #32x7x7\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Conv2d(32, 64, 3, stride = 2, padding=1), #64x4x4\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Conv2d(64, self.hidden_size, 4, stride = 1, padding=0), #64x1x1\n",
    "                                        nn.ReLU(), \n",
    "                                        nn.Flatten())  #hidden_size\n",
    "        \n",
    "\n",
    "        # define mean\n",
    "        self.encoder_mean = nn.Linear(self.hidden_size, self.latent_size)\n",
    "        #define logvar\n",
    "        self.encoder_logvar = nn.Linear(self.hidden_size, self.latent_size)\n",
    "        \n",
    "        self.resize_fc = nn.Linear(self.latent_size, self.hidden_size)\n",
    "        \n",
    "        ## decoder ##\n",
    "        self.Decoder = nn.Sequential(nn.ConvTranspose2d( self.hidden_size, 64, 4, 1, 0), #4x4  (W - 1)S -2P + (K - 1) + 1\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.ConvTranspose2d( 64, 32, 3, 2, 1), #7x7\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.ConvTranspose2d( 32, 16, 4, 2, 1), #14x14\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.ConvTranspose2d( 16, 1, 4, 2, 1), #28x28\n",
    "                                    nn.Sigmoid())\n",
    "\n",
    "    def sample(self, log_var, mean):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mean) # reparametrization trick\n",
    "    \n",
    "    def KL_loss(self, log_var, mean):\n",
    "        return -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Encoder(x).view(x.size(0),-1)\n",
    "        log_var = self.encoder_logvar(x)\n",
    "        mean = self.encoder_mean(x)\n",
    "        z = self.sample(log_var, mean)\n",
    "        \n",
    "        x = self.resize_fc(z).view(z.size(0),self.hidden_size,1,1)\n",
    "        \n",
    "        x = self.Decoder(x)\n",
    "\n",
    "        return x, log_var, mean\n",
    "    \n",
    "    def generate_img(self, z):\n",
    "        x = self.resize_fc(z).view(z.size(0),64,1,1)    \n",
    "        return self.Decoder(x)\n",
    "\n",
    "\n",
    "vae = VAE().to(device)\n",
    "print(vae)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e18ff1d-02d5-4b9c-bfff-6085a233bd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Optimizer and Loss\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
    "\n",
    "# as loss lets use BCE (Binary Cross Entropy)\n",
    "bce_loss = nn.BCELoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74f15bc-3855-4de0-aa35-b02b05ed9ffd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TRAIN!!!\n",
    "\n",
    "PATH = \"./res/vae.pth\"\n",
    "\n",
    "vae.train()\n",
    "\n",
    "for epoch in range(10):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # send to device\n",
    "        data = data.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output, log_var, mean = vae(data)\n",
    "        # loss between the output of AE and original data withoud noise\n",
    "        bce = bce_loss(output, data)\n",
    "        kl = vae.KL_loss(log_var, mean)\n",
    "        loss = bce + kl\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tBCE: {:.6f} KL: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), bce.item(), kl.item()))\n",
    "    \n",
    "    # save model every epoch\n",
    "    #torch.save(vae.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23c943e-d706-4bd3-8b43-cfe7afb38abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load a set of weights\n",
    "vae.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9039200b-3090-4f3b-8bed-1031771390be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "image, target = next(iter(test_loader)) # load one batch\n",
    "image = image.to(device)\n",
    "\n",
    "vae.eval()\n",
    "\n",
    "out, _, _ = vae(image)\n",
    "\n",
    "# input\n",
    "plt.figure()\n",
    "image_grid = torchvision.utils.make_grid(image[:8])\n",
    "plt.imshow(to_pil(image_grid))\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# output\n",
    "plt.figure()\n",
    "out_grid = torchvision.utils.make_grid(out[:8])\n",
    "plt.imshow(to_pil(out_grid))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60051247-9057-4b94-bf93-0da6cab16a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE!!!!\n",
    "z = torch.randn(8,2).to(device)\n",
    "print(z.size())\n",
    "\n",
    "generated_samples = vae.generate_img(z)\n",
    "\n",
    "plt.figure()\n",
    "gen_grid = torchvision.utils.make_grid(generated_samples)\n",
    "plt.imshow(to_pil(gen_grid))\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cd74ce-f571-46b7-b799-94cfb8965196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize latent space\n",
    "image, target = next(iter(test_loader)) # load one batch\n",
    "image = image.to(device)\n",
    "\n",
    "_, log_var, mean = vae(image)\n",
    "\n",
    "# sample noise in the latent space\n",
    "z = vae.sample(log_var, mean)\n",
    "\n",
    "z = z.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea636b5-1a0e-44d4-b6d8-405255726f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "# scale and move the coordinates so they fit [-1; 1] range\n",
    "def scale_to_01_range(x):\n",
    "    # compute the distribution range\n",
    "    value_range = (np.max(x) - np.min(x))\n",
    "\n",
    "    # move the distribution so that it starts from zero\n",
    "    # by extracting the minimal value from all its values\n",
    "    starts_from_zero = x - np.min(x)\n",
    "\n",
    "    # make the distribution fit [-1; 1] by dividing by its range\n",
    "    return 2*(starts_from_zero / value_range) - 1\n",
    "\n",
    "# extract x and y coordinates representing the positions of the images on T-SNE plot\n",
    "tx = z[:, 0]\n",
    "ty = z[:, 1]\n",
    "\n",
    "tx = scale_to_01_range(tx)\n",
    "ty = scale_to_01_range(ty)\n",
    "\n",
    "z[:, 0] = tx\n",
    "z[:, 1] = ty\n",
    "\n",
    "plot_data_np(z, target.numpy(), legend = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8606eac8-16e1-4a84-bb9f-2e28e8da1ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot interpolation of generated sample\n",
    "def plot_reconstructed(vae, r0=(-3, 3), r1=(-3, 3), n=12):\n",
    "    w = 28\n",
    "    img = np.zeros((n*w, n*w))\n",
    "    for i, y in enumerate(np.linspace(*r1, n)):\n",
    "        for j, x in enumerate(np.linspace(*r0, n)):\n",
    "            z = torch.Tensor([[x, y]]).to(device)\n",
    "            x_hat = vae.generate_img(z)\n",
    "            img[(n-1-i)*w:(n-1-i+1)*w, j*w:(j+1)*w] = x_hat.cpu().detach().numpy()\n",
    "    plt.imshow(img, extent=[*r0, *r1])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497af671-bc24-46a6-9071-a9de746b428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reconstructed(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dfcddb-c436-408e-9796-d5d53fce9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex1: train the VAE incrementing the latent space, removing the reparametriztion trick, changing the weights of the KL loss (e.g. 10*kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d008435f-fcf8-4729-8022-f5c9378bfc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex2 train the VAE on Cifar10 (latent space of dim 2 is enough?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
