{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13a2d720-75d0-49f9-b5a4-7bc7190b7a5f",
   "metadata": {},
   "source": [
    "# FINETUNING\n",
    "\n",
    "We exploit a complex model that was already trained on a big dataset (such as ImageNet) and we finetune it to classify our dataset.\n",
    "In order to finetune the following steps are required:\n",
    "- changing the last layer of the pretrained model in order to be on the same output size as our number of classes\n",
    "- freezing (i.e. not training) the initial layers of the model\n",
    "- train the last layers of the models on the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5619e77e-5d0c-449c-9357-d7f99013f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2fa4a9-85f9-41f9-9c09-e7485306dbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfcdbb2-83ae-4351-b30b-6eb1b05ba6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by defining the model to finetune\n",
    "# For this example we will use VGG11\n",
    "import torchvision.models as models\n",
    "\n",
    "# vgg11 = models.vgg11(pretrained=True) # load vgg model pretrained on ImageNet\n",
    "# print(vgg11)\n",
    "\n",
    "# resnet18 = models.resnet18(pretrained = True)\n",
    "# print(resnet18)\n",
    "\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8368e04d-ab32-4257-9818-1818b6678d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to freeze the model layers\n",
    "def set_parameter_requires_grad(model, req_grad = False):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = req_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba82b06f-8a90-4c1a-9d72-aa2c6055fc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes in CIFAR10?\n",
    "NUM_CLASSES = 10\n",
    "# freeze model layers\n",
    "set_parameter_requires_grad(alexnet.features, req_grad = False)\n",
    "# change last layer of the model\n",
    "num_ftrs = alexnet.classifier[6].in_features # get the input dimension of last layer\n",
    "alexnet.classifier[6] = nn.Linear(num_ftrs,NUM_CLASSES)\n",
    "input_size = 224 #model requires this input size\n",
    "\n",
    "alexnet = alexnet.to(device)\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc961595-d4b4-448f-8d6f-8ee9eb4f8a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataset again with correct input size\n",
    "# define transforms\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize(input_size),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) # normalization parameteres tuned on ImageNet mean and var\n",
    "# define batch size\n",
    "batch_size = 4\n",
    "\n",
    "# load train ds\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "# load test ds\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f30546-cb5b-4f40-ac2c-4dfe3f380200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN AGAIN!\n",
    "\n",
    "# define Loss and Optimizer\n",
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(alexnet.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "#for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "running_loss = 0.0\n",
    "for i, data in enumerate(trainloader, 0):\n",
    "    # stop after 2k iterations\n",
    "    if i > 2000:\n",
    "        break\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    inputs, labels = data\n",
    "\n",
    "    # put data on correct device\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = alexnet(inputs) #finetuned model\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    running_loss += loss.item()\n",
    "    if i % 200 == 199:    # print every 200 mini-batches\n",
    "        print(f'[it: {i + 1}] loss: {running_loss / 200:.3f}')\n",
    "        running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58783572-45f2-469f-9785-d841117e5d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to save the model\n",
    "PATH = './res/finetuned_cifar_net.pth'\n",
    "#torch.save(alexnet.state_dict(), PATH)\n",
    "\n",
    "# if you want to load the model\n",
    "alexnet.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9d4562-7df7-43c5-86fa-bf893aa23c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets evaluate the model on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# put the model into evaluation mode\n",
    "alexnet.eval()\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(testloader):\n",
    "        inputs, labels = data\n",
    "        # put data on correct device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = alexnet(inputs)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if i % 200 == 199:    # print every 200 mini-batches\n",
    "            print(f'It: {i}')\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c4c2af-17fa-482c-b4b3-e93c60a957c3",
   "metadata": {},
   "source": [
    "## Features visualization\n",
    "Let's try to visualize the features in the dataset before and after the training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a7f70-2d56-45a8-96e5-fbb8172d559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use TSNE as a tool to visualize high-dimensional data\n",
    "from sklearn.manifold import TSNE #pip install --pre --extra-index https://pypi.anaconda.org/scipy-wheels-nightly/simple scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20193c2-a9a5-4f96-961a-65b6f1027062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# get the features extractors from vgg\n",
    "features_extractor = alexnet.features\n",
    "avg_pool = alexnet.avgpool\n",
    "out_features = alexnet.classifier[:3]\n",
    "# initialize features and labels list\n",
    "features_list = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(testloader):\n",
    "        inputs, labels = data\n",
    "        # put data on correct device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = out_features(torch.flatten(avg_pool(features_extractor(inputs)),1))\n",
    "        #flatten outputs\n",
    "        outputs = outputs.view(outputs.size(0), -1)\n",
    "\n",
    "        current_outputs = outputs.cpu().numpy()\n",
    "        current_labels = labels.cpu().numpy()\n",
    "        # create features list\n",
    "        features_list.append(current_outputs)\n",
    "        labels_list.append(current_labels)\n",
    "        if i % 200 == 199:    # get only 200 batches\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f81ad70-bc5f-451f-b5b8-50121132701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list_cat = np.concatenate(features_list, axis=0)\n",
    "labels_list_cat = np.concatenate(labels_list, axis=0)\n",
    "print(features_list_cat.shape)\n",
    "print(labels_list_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a6b2a4-081c-41e6-9343-fc8b0b76a0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2).fit_transform(features_list_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ce5cac-e77a-46b8-8706-fc92c6b9bdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tsne.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ed4070-de46-476f-b659-44038e408049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale and move the coordinates so they fit [-1; 1] range\n",
    "def scale_to_11_range(x):\n",
    "    # compute the distribution range\n",
    "    value_range = (np.max(x) - np.min(x))\n",
    "\n",
    "    # move the distribution so that it starts from zero\n",
    "    # by extracting the minimal value from all its values\n",
    "    starts_from_zero = x - np.min(x)\n",
    "\n",
    "    # make the distribution fit [-1; 1] by dividing by its range\n",
    "    return 2*(starts_from_zero / value_range) - 1\n",
    "\n",
    "# extract x and y coordinates representing the positions of the images on T-SNE plot\n",
    "tx = tsne[:, 0]\n",
    "ty = tsne[:, 1]\n",
    "\n",
    "tx = scale_to_11_range(tx)\n",
    "ty = scale_to_11_range(ty)\n",
    "\n",
    "tsne[:, 0] = tx\n",
    "tsne[:, 1] = ty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661e55f3-e87f-4941-a90a-cfbb9c9d6381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from res.plot_lib import plot_data, plot_data_np, plot_model, set_default\n",
    "# Initiale default plotting parameters\n",
    "set_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c2bb77-ab47-455f-8f01-acb37d898a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot classes\n",
    "plot_data_np(tsne, labels_list_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaca9b7d-2206-4037-bc44-3e0da6e63fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try with the untrained network\n",
    "alexnet_un = models.alexnet(pretrained=False).to(device) # load vgg model pretrained on ImageNet\n",
    "alexnet_un.eval()\n",
    "\n",
    "# get the features extractors from vgg\n",
    "features_extractor = alexnet_un.features\n",
    "avg_pool = alexnet_un.avgpool\n",
    "out_features = alexnet_un.classifier[:3]\n",
    "# initialize features and labels list\n",
    "features_list = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(testloader):\n",
    "        inputs, labels = data\n",
    "        # put data on correct device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = out_features(torch.flatten(avg_pool(features_extractor(inputs)),1))\n",
    "        #flatten outputs\n",
    "        outputs = outputs.view(outputs.size(0), -1)\n",
    "\n",
    "        current_outputs = outputs.cpu().numpy()\n",
    "        current_labels = labels.cpu().numpy()\n",
    "        # create features list\n",
    "        features_list.append(current_outputs)\n",
    "        labels_list.append(current_labels)\n",
    "        if i % 200 == 199:    # get only 200 el\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01af0d9c-2fad-48e8-9b52-9b08956f84f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list_cat = np.concatenate(features_list, axis=0)\n",
    "labels_list_cat = np.concatenate(labels_list, axis=0)\n",
    "print(features_list_cat.shape)\n",
    "print(labels_list_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cfdbc7-f93f-481a-947e-e45a829883b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2).fit_transform(features_list_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212273ac-8af3-42e7-ac4a-aa19ff8467d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract x and y coordinates representing the positions of the images on T-SNE plot\n",
    "tx = tsne[:, 0]\n",
    "ty = tsne[:, 1]\n",
    "\n",
    "tx = scale_to_11_range(tx)\n",
    "ty = scale_to_11_range(ty)\n",
    "\n",
    "tsne[:, 0] = tx\n",
    "tsne[:, 1] = ty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836714d-a654-45f1-8586-8f6bac3f34f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot classes\n",
    "plot_data_np(tsne, labels_list_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554b9ff7-e693-4bf7-afb8-9204b7df85e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex1: do the finetuning using a resnet18 and vgg11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a13197-038d-4e8a-9d66-12f99bde28c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex2: try to train the full model (both pretrained and not pretrained) and not just the last layers.\n",
    "# How the results are different? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216adb1-2b46-4d98-9920-c66721bee384",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
